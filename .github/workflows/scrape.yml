name: JobThai Scraper Uni

# Schedule and manual trigger
on:
  workflow_dispatch:       # 1. Manual run button
  schedule:
    - cron: '0 17 * * *'   # 2. Auto-run daily at 00:00 Thailand time (UTC 17:00)

jobs:
  scrape_job:
    runs-on: ubuntu-latest
    
    # Set Timezone to Thailand
    env:
      TZ: 'Asia/Bangkok'

    steps:
      # 1. Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      # 3. Install Google Chrome (Latest stable version)
      - name: Install Google Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      # 4. Install Xvfb and dependencies (แก้ไขชื่อแพ็กเกจสำหรับ Ubuntu 24.04)
      - name: Install Xvfb and Dependencies
        run: |
          sudo apt-get update
          # เปลี่ยน libasound2 เป็น libasound2t64 ตามมาตรฐานใหม่ของ Ubuntu Noble
          sudo apt-get install -y xvfb libnss3 libasound2t64 libgbm1

      # 5. Install Python libraries
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Ensure all necessary libraries are installed
          pip install pandas undetected-chromedriver python-dotenv pyyaml rich thefuzz python-dateutil gspread oauth2client fake-useragent
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      # 6. Create Config & Env Files
      - name: Create Config & Env Files
        env:
          DATA_COMP: ${{ secrets.COMPETITORS_DATA }}
          DATA_CLIENT: ${{ secrets.CLIENTS_DATA }}
          DATA_TIER1: ${{ secrets.TIER1_DATA }}
        run: |
          # Create yaml files
          echo "$DATA_COMP" > compe.yaml
          echo "$DATA_CLIENT" > co.yaml
          echo "$DATA_TIER1" > tier1.yaml
          
          # Create User.env
          touch User.env
          echo "JOBTHAI_USER=${{ secrets.JOBTHAI_USER }}" >> User.env
          echo "JOBTHAI_PASS=${{ secrets.JOBTHAI_PASS }}" >> User.env
          echo "EMAIL_SENDER=${{ secrets.EMAIL_SENDER }}" >> User.env
          echo "EMAIL_PASSWORD=${{ secrets.EMAIL_PASSWORD }}" >> User.env
          echo "EMAIL_RECEIVER=${{ secrets.EMAIL_RECEIVER }}" >> User.env

      # 7. Run Scraper with Xvfb
      - name: Run Scraper with Xvfb
        env:
          JOBTHAI_USER: ${{ secrets.JOBTHAI_USER }}
          JOBTHAI_PASS: ${{ secrets.JOBTHAI_PASS }}
          EMAIL_SENDER: ${{ secrets.EMAIL_SENDER }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_RECEIVER: ${{ secrets.EMAIL_RECEIVER }}
          COOKIES_JSON: ${{ secrets.COOKIES_JSON }}
          GITHUB_EVENT_NAME: ${{ github.event_name }}
          G_SHEET_KEY: ${{ secrets.G_SHEET_KEY }}
          G_SHEET_NAME: ${{ secrets.G_SHEET_NAME }}
        run: |
          xvfb-run --auto-servernum --server-args="-screen 0 1920x1080x24" python Git1.py

      # 8. Upload Results
      - name: Upload Results (CSV & Images)
        if: always() # Run even if the previous step fails
        uses: actions/upload-artifact@v4
        with:
          name: scraper-results
          path: |
            *.csv
            resume_images/
            *.png
          retention-days: 1
